{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 4334/5334 Programming Assignment 1 (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: 11:59pm Central Time, Friday, November 2, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions on this assignment are written in an .ipynb file. You can use the following commands to install the Jupyter notebook viewer. \"pip\" is a command for installing Python packages. You are required to use \"Python 3.6.x\" (any version of Python equal to or greater than version Python 3.6.0.) in this project.\n",
    "\n",
    "    pip install jupyter\n",
    "\n",
    "To run the Jupyter notebook viewer, use the following command:\n",
    "\n",
    "    jupyter notebook P1.ipynb\n",
    "\n",
    "The above command will start a webservice at http://localhost:8888/ and display the instructions in the '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This assignment must be done individually. You must implement the whole assignment by yourself. Academic dishonety will have serious consequences.\n",
    "* You can discuss topics related to the assignment with your fellow students. But you are not allowed to discuss/share your solution and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the files for this assignment can be downloaded from Blackboard (\"Course Materials\" > \"Programming Assignments\" > \"Programming Assignment 1 (P1)\" > \"Attached Files\").\n",
    "\n",
    "#### 1. This instruction file itself \"P1.ipynb\"\n",
    "#### 2. Data file \"debate.txt\"\n",
    "We use the transcript of the latest Texas Senate race debate between Senator Ted Cruz and Representative Beto O'Rourke, which took place on October 16, 2018. We pre-processed the transcript and provide you a text file debate.txt. In the file each paragraph is a segement of the debate from one of the candidates or moderators. \n",
    "#### 3. Sample results \"sampleresults.txt\"\n",
    "#### 4. Grading rubrics \"rubrics.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will test your code under the particular version of Python 3.6.x. So make sure you develop your code using the same version. \n",
    "\n",
    "2. You are free to use anything from the Python Standard Library that comes with Python 3.6.x (https://docs.python.org/3.6/library/).\n",
    "\n",
    "3. You are expected to use several modules in NLTK--a natural language processing toolkit for Python. NLTK doesn't come with Python by default. You need to install it and \"import\" it in your .py file. NLTK's website (http://www.nltk.org/index.html) provides a lot of useful information, including a book http://www.nltk.org/book/, as well as installation instructions (http://www.nltk.org/install.html).\n",
    "\n",
    "4. <b>You are NOT allowed to use any non-standard Python package, except NLTK.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Description of Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You code should accomplish the following tasks:\n",
    "\n",
    "(1) <b>Read</b> the text file debate.txt. This is the transcript of the latest Texas Senate race debate between Ted Cruz and Beto O'Rourke. The following code does it. \n",
    "\n",
    "In this assignment we ignore the difference between lower and upper cases. So convert the text to lower case before you do anything else with the text. For a query, also convert it to lower case before doing anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './debate.txt'\n",
    "file = open(filename, \"r\", encoding='UTF-8')\n",
    "doc = file.readlines()\n",
    "file.close() \n",
    "#for line in doc: \n",
    "    #print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) <b>Tokenize</b> the content of the file. For this, you need a tokenizer. For example, the following piece of code uses a regular expression tokenizer to return all course numbers in a string. Play with it and edit it. You can change the regular expression and the string to observe different output results. \n",
    "\n",
    "For tokenizing the Texas Senate debate transcript, let's all use RegexpTokenizer(r'[a-zA-Z]+'). What tokens will it produce? What limitations does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSE4334', 'CSE5334', 'IE3013']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-Z]{2,3}[1-9][0-9]{3,3}')\n",
    "tokens = tokenizer.tokenize(\"CSE4334 and CSE5334 are taught together. IE3013 is an undergraduate course.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Perform <b>stopword removal</b> on the obtained tokens. NLTK already comes with a stopword list, as a corpus in the \"NLTK Data\" (http://www.nltk.org/nltk_data/). You need to install this corpus. Follow the instructions at http://www.nltk.org/data.html. You can also find the instruction in this book: http://www.nltk.org/book/ch01.html (Section 1.2 Getting Started with NLTK). Basically, use the following statements in Python interpreter. A pop-up window will appear. Click \"Corpora\" and choose \"stopwords\" from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the stopword list is downloaded, you will find a file \"english\" in folder nltk_data/corpora/stopwords, where folder nltk_data is the download directory in the step above. The file contains 179 stopwords. nltk.corpus.stopwords will give you this list of stopwords. Try the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(sorted(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Also perform <b>stemming</b> on the obtained tokens. NLTK comes with a Porter stemmer. Try the following code and learn how to use the stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('studying'))\n",
    "print(stemmer.stem('vector'))\n",
    "print(stemmer.stem('entropy'))\n",
    "print(stemmer.stem('hispanic'))\n",
    "print(stemmer.stem('ambassador'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Using the tokens, compute the <b>TF-IDF vector</b> for each <b>paragraph</b>. <b>In this assignment, for calculating inverse document frequency, treat debate.txt as the whole corpus and the paragraphs as documents.</b> That is also why we ask you to compute the TF-IDF vectors separately for all the paragraphs, one vector per paragraph. \n",
    "\n",
    "Use the following equation that we learned in the lectures to calculate the term weights, in which $t$ is a token and $d$ is a document (i.e., paragraph):  $$w_{t,d} = (1+log_{10}{tf_{t,d}})\\times(log_{10}{\\frac{N}{df_t}}).$$ Note that the TF-IDF vectors should be normalized (i.e., their lengths should be 1). \n",
    "\n",
    "Represent a TF-IDF vector by a dictionary. The following is a sample TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'sanction': 0.014972337775895645, 'lack': 0.008576372825970286, 'regret': 0.009491784747267843, 'winter': 0.030424375278541155}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Given a query string, calculate the query vector. Compute the <b>cosine similarity</b> between the query vector and the paragraphs in the transcript. Return the paragraph that attains the highest cosine similarity score. In calculating the query vector, the vector is also to be normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What to Submit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit through Blackboard your source code in a single .py file. You can define as many functions as you want, but the file must define at least the following functions. \n",
    "\n",
    "* <b>getidf(token)</b>: return the inverse document frequency of a token. If the token doesn't exist in the corpus, return -1. Hence, if the parameter \"token\" has any upper case character, the result will be -1, since we converted everything in the corpus to lower case in our first step of processing the data. For the same reason, the function expects the parameter 'token' is already stemmed. If it is not stemmed and its stemmed version is different, the result will be -1. Note the differences between getidf(\"hispan\") and getidf(\"hispanic\"). This also means you should not perform stemming inside this function. \n",
    "\n",
    "* <b>getqvec(qstring)</b>: return the query vector for a query string. \n",
    "\n",
    "* <b>query(qstring)</b>: return the paragraph in the transcript that has the highest cosine similarity score with respect to 'qstring'. Return its score too. The output format should be as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the paragraph\n",
    "\n",
    "the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all paragraphs have zero scores, return the following message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Match\n",
    "\n",
    "0.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample Results \n",
    "\n",
    "The file \"sampleresults.txt\" provides multiple sample results of calling the aforementioned three functions.\n",
    "\n",
    "<b>We use a script to automatically grade. Make sure your code produces identical results in idential format. Or else you won't get points. And apparently make sure your program runs.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Grading Rubrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program will be evaluated on correctness, efficiency, and code quality.\n",
    "\n",
    "<b>Make sure to thoroughly understand the grading rubrics in file \"rubrics.txt\".</b>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
